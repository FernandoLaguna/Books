{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cddb16",
   "metadata": {},
   "source": [
    " # Predicting Book Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c45e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "from path import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40bea4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "      <th>Pages</th>\n",
       "      <th>Rating Count</th>\n",
       "      <th>Language_Dummy</th>\n",
       "      <th>Size_Dummy</th>\n",
       "      <th>Famous_Dummy</th>\n",
       "      <th>Categories_Dummy</th>\n",
       "      <th>Serie_Dummy</th>\n",
       "      <th>Rating_Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9780439358071</td>\n",
       "      <td>4.49</td>\n",
       "      <td>7.38</td>\n",
       "      <td>870</td>\n",
       "      <td>1996446</td>\n",
       "      <td>English</td>\n",
       "      <td>Big-Publisher</td>\n",
       "      <td>Mid-Fame</td>\n",
       "      <td>Top_Category</td>\n",
       "      <td>Serie</td>\n",
       "      <td>High_Rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9780316015844</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.10</td>\n",
       "      <td>498</td>\n",
       "      <td>4367341</td>\n",
       "      <td>English</td>\n",
       "      <td>Big-Publisher</td>\n",
       "      <td>Low-Fame</td>\n",
       "      <td>Other</td>\n",
       "      <td>Serie</td>\n",
       "      <td>Low_Rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9780345538376</td>\n",
       "      <td>4.59</td>\n",
       "      <td>21.15</td>\n",
       "      <td>1728</td>\n",
       "      <td>97731</td>\n",
       "      <td>English</td>\n",
       "      <td>Big-Publisher</td>\n",
       "      <td>High-Fame</td>\n",
       "      <td>Top_Category</td>\n",
       "      <td>Serie</td>\n",
       "      <td>High_Rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9780393978896</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.73</td>\n",
       "      <td>464</td>\n",
       "      <td>1041597</td>\n",
       "      <td>English</td>\n",
       "      <td>Big-Publisher</td>\n",
       "      <td>Low-Fame</td>\n",
       "      <td>Top_Category</td>\n",
       "      <td>Other</td>\n",
       "      <td>Low_Rating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9780142437209</td>\n",
       "      <td>4.11</td>\n",
       "      <td>5.46</td>\n",
       "      <td>532</td>\n",
       "      <td>1328143</td>\n",
       "      <td>English</td>\n",
       "      <td>Big-Publisher</td>\n",
       "      <td>Low-Fame</td>\n",
       "      <td>Top_Category</td>\n",
       "      <td>Other</td>\n",
       "      <td>High_Rating</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ISBN  Rating  Price  Pages  Rating Count Language_Dummy  \\\n",
       "0  9780439358071    4.49   7.38    870       1996446        English   \n",
       "1  9780316015844    3.59   2.10    498       4367341        English   \n",
       "2  9780345538376    4.59  21.15   1728         97731        English   \n",
       "3  9780393978896    3.84   2.73    464       1041597        English   \n",
       "4  9780142437209    4.11   5.46    532       1328143        English   \n",
       "\n",
       "      Size_Dummy Famous_Dummy Categories_Dummy Serie_Dummy  \\\n",
       "0  Big-Publisher     Mid-Fame     Top_Category       Serie   \n",
       "1  Big-Publisher     Low-Fame            Other       Serie   \n",
       "2  Big-Publisher    High-Fame     Top_Category       Serie   \n",
       "3  Big-Publisher     Low-Fame     Top_Category       Other   \n",
       "4  Big-Publisher     Low-Fame     Top_Category       Other   \n",
       "\n",
       "  Rating_Classification  \n",
       "0           High_Rating  \n",
       "1            Low_Rating  \n",
       "2           High_Rating  \n",
       "3            Low_Rating  \n",
       "4           High_Rating  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Import the dataset\n",
    "data = Path('books_clean.csv')\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "# We can see from the preview of the DataFrame that multiple variables (also called features), such as the isbn13,\n",
    "#published_year, average_rating, num_pages, ratings_count, can be used to predict the outcome: whether a book will have a good \n",
    "#rating (1) or will not (0) based on the fact that an average rating below of 4.5 will not likely be successful\n",
    "\n",
    "df.loc[df['Rating'] <= 4.1, 'Rating_Classification'] = 'Low_Rating' \n",
    "df.loc[df['Rating'] > 4.1, 'Rating_Classification'] = 'High_Rating'\n",
    "\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40a038d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ISBN                       int64\n",
       "Rating                   float64\n",
       "Price                    float64\n",
       "Pages                      int64\n",
       "Rating Count               int64\n",
       "Language_Dummy            object\n",
       "Size_Dummy                object\n",
       "Famous_Dummy              object\n",
       "Categories_Dummy          object\n",
       "Serie_Dummy               object\n",
       "Rating_Classification     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will check for the variables from all columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e1be63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the current NaN values from the dataframe\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b16ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all the NaN values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33abbfec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recount all the NaN values to make sure they are dropped\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb8b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use method to convert String to int\n",
    "def rating(x):\n",
    "    if x == 'Low_Rating':\n",
    "        return 0\n",
    "    if x == 'High_Rating':\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d58b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the method to the rating_classification column\n",
    "df['Rating_Classification'] = df['Rating_Classification'].apply(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9bbf802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use method to convert String to int\n",
    "df=pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d950b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "      <th>Pages</th>\n",
       "      <th>Rating Count</th>\n",
       "      <th>Rating_Classification</th>\n",
       "      <th>Language_Dummy_English</th>\n",
       "      <th>Language_Dummy_Other</th>\n",
       "      <th>Size_Dummy_Big-Publisher</th>\n",
       "      <th>Size_Dummy_Mid-Publisher</th>\n",
       "      <th>Size_Dummy_Small-Publisher</th>\n",
       "      <th>Famous_Dummy_High-Fame</th>\n",
       "      <th>Famous_Dummy_Low-Fame</th>\n",
       "      <th>Famous_Dummy_Mid-Fame</th>\n",
       "      <th>Categories_Dummy_Other</th>\n",
       "      <th>Categories_Dummy_Top_Category</th>\n",
       "      <th>Serie_Dummy_Other</th>\n",
       "      <th>Serie_Dummy_Serie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>9780441731183</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.89</td>\n",
       "      <td>341</td>\n",
       "      <td>6092</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>9780811213769</td>\n",
       "      <td>4.30</td>\n",
       "      <td>3.59</td>\n",
       "      <td>99</td>\n",
       "      <td>599</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>9780553562606</td>\n",
       "      <td>4.08</td>\n",
       "      <td>3.12</td>\n",
       "      <td>518</td>\n",
       "      <td>5356</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>9781400044870</td>\n",
       "      <td>4.02</td>\n",
       "      <td>3.59</td>\n",
       "      <td>320</td>\n",
       "      <td>4455</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>9780553273861</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.52</td>\n",
       "      <td>291</td>\n",
       "      <td>580</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ISBN  Rating  Price  Pages  Rating Count  \\\n",
       "1926  9780441731183    3.78   3.89    341          6092   \n",
       "1927  9780811213769    4.30   3.59     99           599   \n",
       "1928  9780553562606    4.08   3.12    518          5356   \n",
       "1929  9781400044870    4.02   3.59    320          4455   \n",
       "1930  9780553273861    3.64   3.52    291           580   \n",
       "\n",
       "      Rating_Classification  Language_Dummy_English  Language_Dummy_Other  \\\n",
       "1926                      0                       1                     0   \n",
       "1927                      1                       1                     0   \n",
       "1928                      0                       1                     0   \n",
       "1929                      0                       1                     0   \n",
       "1930                      0                       1                     0   \n",
       "\n",
       "      Size_Dummy_Big-Publisher  Size_Dummy_Mid-Publisher  \\\n",
       "1926                         1                         0   \n",
       "1927                         1                         0   \n",
       "1928                         1                         0   \n",
       "1929                         1                         0   \n",
       "1930                         1                         0   \n",
       "\n",
       "      Size_Dummy_Small-Publisher  Famous_Dummy_High-Fame  \\\n",
       "1926                           0                       0   \n",
       "1927                           0                       0   \n",
       "1928                           0                       0   \n",
       "1929                           0                       0   \n",
       "1930                           0                       0   \n",
       "\n",
       "      Famous_Dummy_Low-Fame  Famous_Dummy_Mid-Fame  Categories_Dummy_Other  \\\n",
       "1926                      0                      1                       0   \n",
       "1927                      1                      0                       0   \n",
       "1928                      1                      0                       0   \n",
       "1929                      1                      0                       0   \n",
       "1930                      0                      1                       0   \n",
       "\n",
       "      Categories_Dummy_Top_Category  Serie_Dummy_Other  Serie_Dummy_Serie  \n",
       "1926                              1                  0                  1  \n",
       "1927                              1                  1                  0  \n",
       "1928                              1                  0                  1  \n",
       "1929                              1                  1                  0  \n",
       "1930                              1                  1                  0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a new dataframe with the target variables\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5d523",
   "metadata": {},
   "source": [
    "##  Separate the Features (X) from the Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c4fd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Outcome column is defined as y, or the target.\n",
    "#X, or features, is created by dropping the Outcome column from the DataFrame.\n",
    "\n",
    "y = df[\"Rating_Classification\"]\n",
    "X = df.drop(columns=[\"Rating_Classification\",\"ISBN\",\"Rating\",\"Rating Count\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a345b",
   "metadata": {},
   "source": [
    " ## Split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d92082b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1448, 14)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We first split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d937c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "743bb8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examining the shape of the training set with X_train.shape returned (1636,5), meaning that there are 1636 samples (rows) and \n",
    "#five features (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3ae9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step was to create a logistic regression model with the specified arguments for solver, max_iter, and random_state\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs',\n",
    "                                max_iter=200,\n",
    "                                random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1580c600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200, random_state=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we trained the model with the training data\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a4e996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Prediction  Actual\n",
       "463           0       1\n",
       "464           0       0\n",
       "465           0       1\n",
       "466           0       0\n",
       "467           0       1\n",
       "468           0       0\n",
       "469           0       0\n",
       "470           0       0\n",
       "471           0       0\n",
       "472           0       0\n",
       "473           0       0\n",
       "474           0       0\n",
       "475           0       0\n",
       "476           0       1\n",
       "477           0       0\n",
       "478           0       0\n",
       "479           0       0\n",
       "480           0       1\n",
       "481           1       1\n",
       "482           0       1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To create predictions for y-values, we used the X_test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "results = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "results.tail(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab3587f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6997929606625258\n"
     ]
    }
   ],
   "source": [
    "#The final step is to measure the accuracy of the logistic regression model created\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "283d57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking into account that the accuracy score is simply the percentage of predictions that are correct. In this case, \n",
    "#the model's accuracy score was 0.9834, meaning that the model was correct 98.34% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04bc3140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>313</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>130</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0          313           15\n",
       "Actual 1          130           25"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  import the relevant modules for validation and print the confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "matrix_df = pd.DataFrame(\n",
    "    matrix, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a9be58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.95      0.81       328\n",
      "           1       0.62      0.16      0.26       155\n",
      "\n",
      "    accuracy                           0.70       483\n",
      "   macro avg       0.67      0.56      0.53       483\n",
      "weighted avg       0.68      0.70      0.63       483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report of sensitivity, precission and F1. La línea 0 se aplica al predictor de diabetes como hipótesis alternativa\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d3e0e",
   "metadata": {},
   "source": [
    "# Single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868862f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "46/46 [==============================] - 1s 2ms/step - loss: 0.7929 - accuracy: 0.6305\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.7609 - accuracy: 0.6381\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.7360 - accuracy: 0.6450\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.7146 - accuracy: 0.6630\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.6976 - accuracy: 0.6782\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.6838 - accuracy: 0.6823\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.6721 - accuracy: 0.6927\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.6623 - accuracy: 0.6913\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.6541 - accuracy: 0.6920\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6468 - accuracy: 0.6878\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6404 - accuracy: 0.6872\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.6878\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6300 - accuracy: 0.6906\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6256 - accuracy: 0.6927\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6218 - accuracy: 0.6968\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6185 - accuracy: 0.7003\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6156 - accuracy: 0.7051\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6131 - accuracy: 0.7086\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6108 - accuracy: 0.7079\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.6088 - accuracy: 0.7044\n",
      "Epoch 21/200\n",
      " 1/46 [..............................] - ETA: 0s - loss: 0.6543 - accuracy: 0.6250"
     ]
    }
   ],
   "source": [
    "# Define the basic neural network model\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"relu\", input_dim=len(X.columns)))\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the Sequential model together and customize metrics\n",
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn_model.fit(X_train_scaled, y_train, epochs=200)\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6290d9d4",
   "metadata": {},
   "source": [
    "# Prediction of testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f26d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the classification of a new set of data\n",
    "new_X_scaled = X_test_scaled\n",
    "y_prediction=(nn_model.predict(new_X_scaled) > 0.5).astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43980895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataframe for predictions\n",
    "pred_df = pd.DataFrame(data=y_prediction, columns=[\"Prediction\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ff6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataframe for y_test\n",
    "single_results_df = pd.DataFrame({\"Actual\": y_test}).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataframe merging testing and prediction\n",
    "neuronal_pred_df = pd.merge(single_results_df, pred_df,left_index=True, right_index=True)\n",
    "neuronal_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47702afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "matrix = confusion_matrix(y_test, y_prediction)\n",
    "matrix_df = pd.DataFrame(\n",
    "    matrix, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f272ec1",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X.columns)\n",
    "hidden_nodes_layer1 = 4\n",
    "hidden_nodes_layer2 = 4\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16952515",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = nn.fit(X_train,y_train,epochs=300)\n",
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the classification of a new set of data\n",
    "new_X_scaled = X_test_scaled\n",
    "y_prediction=(nn.predict(new_X_scaled) > 0.5).astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5612cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataframe for predictions\n",
    "pred_df = pd.DataFrame(data=y_prediction, columns=[\"Prediction\"])\n",
    "# Create a Dataframe for y_test\n",
    "single_results_df = pd.DataFrame({\"Actual\": y_test}).reset_index(drop=True)\n",
    "# Create a Dataframe merging testing and prediction\n",
    "neuronal_pred_df = pd.merge(single_results_df, pred_df,left_index=True, right_index=True)\n",
    "neuronal_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "matrix = confusion_matrix(y_test, y_prediction)\n",
    "matrix_df = pd.DataFrame(\n",
    "    matrix, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1902cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e576aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d296693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dccbe4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
